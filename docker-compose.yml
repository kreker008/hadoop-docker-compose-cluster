version: "3.7"

x-hadoop-build_env:
    &hadoop-build-env
    HADOOP_GROUP: ${HADOOP_GROUP}
    HADOOP_USER: ${HADOOP_USER}
    HADOOP_USER_PASS: ${HADOOP_USER_PASS}
    HADOOP_ARCHIVE_URL: ${HADOOP_ARCHIVE_URL}
    HADOOP_DATANODES: ${HADOOP_DATANODES}
    SPARK_ARCHIVE_URL: ${SPARK_ARCHIVE_URL}
    SPARK_HOME: ${SPARK_HOME}
    HIVE_HOME: ${HIVE_HOME}
    HIVE_ARCHIVE_URL: ${HIVE_ARCHIVE_URL}
    POSTGRES_JDBC_URL: ${POSTGRES_JDBC_URL}

x-hadoop-services-hosts:
    &hadoop-services-hosts
    extra_hosts:
        - "hivemetastore:172.19.34.34"
        - "namenode:172.19.34.35"
        - "datanode1:172.19.34.36"
        - "datanode2:172.19.34.37"
        - "datanode3:172.19.34.38"
        - "huedb:172.19.34.39"
        - "hue:172.19.34.40"
    
services:
    hadoop_core_image:
        build: 
            context: .
            dockerfile: ./images/hadoop_core/Dockerfile
            args:
                <<: *hadoop-build-env
        image: hadoop_core
    hivemetastore:
        image: postgres:16.2
        container_name: hivemetastore
        environment:
            POSTGRES_DB: ${HIVEMETASTORE_DB}
            POSTGRES_USER: ${HIVEMETASTORE_DB_USERNAME}
            POSTGRES_PASSWORD: ${HIVEMETASTORE_DB_PASS}
        volumes:
            - ./volumes/hivemetastore:/var/lib/postgresql/data
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.34
        <<: *hadoop-services-hosts
    namenode:
        build:
            context: .
            dockerfile: ./images/namenode/Dockerfile
            args:
                <<: *hadoop-build-env
        image: namenode_image
        depends_on: 
            - hivemetastore
            - hadoop_core_image
        env_file: .env 
        container_name: "namenode" 
        ports:
            #ssh
            - 43156:22
            #HDFS RPC
            - 8020:${NAMENODE_RPC_PORT}
            #HDFS ui
            - 9870:9870
            #YARN ui
            - 8088:8088
            #Master spark ui
            - 8080:8080
            #Master spark - spark protocl port
            - 7077:7077
            #HIVE(hiveserver2) jdbc port
            - 10000:10000
            #Spark history server
            - 18080:18080
            #Livy REST
            - 8998:8998
        volumes:
            - ./volumes/configs/:/root/utils/configs/
            - ./volumes/namenodehdfs/:/hadoop/hdfs/namenode
        command:
            - -c
            - '/root/utils/configs/startnamenode.sh;
               tail -F anything'
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.35
        <<: *hadoop-services-hosts
    datanode1:
        image: hadoop_core
        depends_on:
            - hadoop_core_image
            - namenode
        env_file: .env
        container_name: "datanode1" 
        volumes:
            - ./volumes/configs/:/root/utils/configs/
            - ./volumes/datanodehdfs1/:/hadoop/hdfs/datanode
        command:
            - -c
            - '/root/utils/configs/startdatanode.sh;
               tail -F anything'
        ports:       
            - 8042:8042
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.36
        <<: *hadoop-services-hosts
    datanode2:
        image: hadoop_core
        depends_on:
            - hadoop_core_image
            - namenode
        env_file: .env
        container_name: "datanode2" 
        volumes:
            - ./volumes/configs/:/root/utils/configs/
            - ./volumes/datanodehdfs2/:/hadoop/hdfs/datanode
        command:
            - -c
            - '/root/utils/configs/startdatanode.sh;
               tail -F anything'
        ports:       
            - 8043:8042
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.37
        <<: *hadoop-services-hosts
    datanode3:
        image: hadoop_core
        depends_on:
            - hadoop_core_image
            - namenode
        env_file: .env
        container_name: "datanode3" 
        volumes:
            - ./volumes/configs/:/root/utils/configs/
            - ./volumes/datanodehdfs3/:/hadoop/hdfs/datanode
        command:
            - -c
            - '/root/utils/configs/startdatanode.sh;
               tail -F anything'
        ports:       
            - 8044:8042
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.38
        <<: *hadoop-services-hosts
    huedb:
        image: postgres:16.2
        container_name: huedb
        volumes:
          - ./volumes/huedb:/var/lib/postgresql/data
        environment:
          POSTGRES_USER: hue
          POSTGRES_PASSWORD: hue
          POSTGRES_DB: hue
        command: -p 5433
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.39
    hue:
        image: gethue/hue:20240410-140101
        container_name: hue
        ports:
          - "8888:8888"
        volumes:
          - ./volumes/hue/hue.ini:/usr/share/hue/desktop/conf/hue.ini
        depends_on:
          - huedb
        restart: on-failure
        networks:
            hadoop_net:
                ipv4_address: 172.19.34.40
        <<: *hadoop-services-hosts
                
networks:
  hadoop_net:
    ipam:
      driver: default
      config:
        - subnet: 172.19.34.0/24